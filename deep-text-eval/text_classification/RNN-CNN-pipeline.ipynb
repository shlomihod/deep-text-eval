{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! pip install numpy\\n! pip install pandas\\n! pip install nltk\\n! pip install keras\\n! pip install matplotlib\\n! pip install sklearn\\n! pip install plotly\\n! pip install gensim\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "\"\"\"\n",
    "! pip install numpy\n",
    "! pip install pandas\n",
    "! pip install nltk\n",
    "! pip install keras\n",
    "! pip install matplotlib\n",
    "! pip install sklearn\n",
    "! pip install plotly\n",
    "! pip install gensim\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vageesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, merge, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model,load_model\n",
    "\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace\n",
    "import inspect\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (7349, 2)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_hdf(\"weebit.h5\",\"text_df\")[['text','y']]\n",
    "# Dropping null values\n",
    "df.dropna(inplace=True)\n",
    "# Converting class labels to int dtype\n",
    "df['y'] = df['y'].astype(int)\n",
    "print(\"Shape of the dataset:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text data\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"german\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)  \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('german')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your web brows does not hav javascript switche...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the most radical chang styl for women's clothi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'i'v mad out will; i'v left myself'+'i'v mad o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kennedy announc naval blockad cuba. b52 nuclea...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>compar what jennif and michael said this rol p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y\n",
       "0  your web brows does not hav javascript switche...  4\n",
       "2  the most radical chang styl for women's clothi...  4\n",
       "5  'i'v mad out will; i'v left myself'+'i'v mad o...  4\n",
       "6  kennedy announc naval blockad cuba. b52 nuclea...  4\n",
       "7  compar what jennif and michael said this rol p...  4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37271 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Because of the computational expenses, I am using the top 20000 unique words. \n",
    "# At first, the text is tokenized and then convert those into sequences. \n",
    "# I have kept 50 words to limit the number of words in each comment.\n",
    "vocabulary_size = 20000\n",
    "\n",
    "# Initializing Tokenizer from keras\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "# Fitting text on the tokenizer\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "# Converting text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "# Finding unique tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Padding sequences to the length of MAX_SEQUENCE_LENGTH\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "labels = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels from the corpora: [4 2 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "unique_class_labels = df['y'].unique()\n",
    "print(\"class labels from the corpora:\",unique_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to modify elements of a dictionary\n",
    "class make_dict(dict):\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of sentences in the Corpora is 7349\n",
      "No of sentences in class label 4 is 4525\n",
      "No of sentences in class label 2 is 798\n",
      "No of sentences in class label 1 is 788\n",
      "No of sentences in class label 0 is 607\n",
      "No of sentences in class label 3 is 631\n"
     ]
    }
   ],
   "source": [
    "# Printing the total number of sentences in the corpora\n",
    "print(\"Total no of sentences in the Corpora is\",df.shape[0])\n",
    "# Getting count of sentences for every class\n",
    "count_list = []\n",
    "for class_label in df['y'].unique():\n",
    "    print(\"No of sentences in class label\",str(class_label) + \" is \" + str(df[df['y']==class_label].shape[0]))\n",
    "    count_list.append((str(class_label),int(df[df['y']==class_label].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data into dictionary for plotting\n",
    "no_of_sentence = make_dict()\n",
    "for values in count_list:\n",
    "    no_of_sentence[values[0]] = values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e2d684f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(no_of_sentence.keys()), no_of_sentence.values(), color='g')\n",
    "plt.suptitle(no_of_sentence.keys(), fontsize=10)\n",
    "plt.xlabel('Class labels', fontsize=10)\n",
    "plt.ylabel('No of Sentences', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (7349, 50)\n",
      "Shape of label tensor: (7349, 5)\n"
     ]
    }
   ],
   "source": [
    "# Getting the labels and features data\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the splitting index for training and testing data\n",
    "SPLIT_RATIO = 0.25\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "validation_samples = int(SPLIT_RATIO * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the testing and training dataset\n",
    "X_train = data[:-validation_samples]\n",
    "y_train = labels[:-validation_samples]\n",
    "X_test = data[-validation_samples:]\n",
    "y_test = labels[-validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Using Convolutional neural network with multiple filter sizes(Deutsche lernen accuracy = 98.16%,Weebit Corpus accuracy=58.02-77.35%,GEO Corpus accuracy=86.84%)\n",
    "# (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network starts with an embedding layer. The layer lets the system expand each token to a more massive vector, allowing the network to represent a word in a meaningful way. The layer takes 20000 as the first argument, which is the size of our vocabulary, and 100 as the second input parameter, which is the dimension of the embeddings. The third parameter is the input_length of 50, which is the length of each text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the globals\n",
    "max_input_length = 50\n",
    "vocabulary_size = 20000\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = (2,4,5,8)\n",
    "dropout_prob = [0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Convolution layer\n",
    "graph_in = Input(shape=(max_input_length, embedding_dim))\n",
    "convs = []\n",
    "avgs = []\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    conv = Conv1D(nb_filter=32,filter_length=fsz,border_mode='valid',activation='relu',subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length = max_input_length - fsz + 1)(conv)\n",
    "    flattenMax = Flatten()(pool)\n",
    "    convs.append(flattenMax)\n",
    "\n",
    "if len(filter_sizes)>1:\n",
    "    out = concatenate(convs,axis=-1)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out, name=\"graphModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_cnn = Sequential()\\nmodel_cnn.add(Embedding(input_dim=vocabulary_size, output_dim = embedding_dim,input_length = max_input_length,trainable=True))\\nmodel_cnn.add(Dropout(dropout_prob[0]))\\nmodel_cnn.add(graph)\\nmodel_cnn.add(Dense(256))\\nmodel_cnn.add(Dropout(dropout_prob[1]))\\nmodel_cnn.add(Activation(\\'relu\\'))\\nmodel_cnn.add(Dense(y_train.shape[1]))\\nmodel_cnn.add(Activation(\\'softmax\\'))\\nmodel_cnn.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'accuracy\\'])\\nprint(\"model fitting - CNN network\")\\nmodel_cnn.summary()\\n# Training the model\\nmodel_cnn.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\\n# Saving the model\\nmodel_cnn.save(\"simple_cnn.h5\")'"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim=vocabulary_size, output_dim = embedding_dim,input_length = max_input_length,trainable=True))\n",
    "model_cnn.add(Dropout(dropout_prob[0]))\n",
    "model_cnn.add(graph)\n",
    "model_cnn.add(Dense(256))\n",
    "model_cnn.add(Dropout(dropout_prob[1]))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Dense(y_train.shape[1]))\n",
    "model_cnn.add(Activation('softmax'))\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "print(\"model fitting - CNN network\")\n",
    "model_cnn.summary()\n",
    "# Training the model\n",
    "model_cnn.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_cnn.save(\"simple_cnn.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_cnn = load_model(\"simple_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.35%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_model_cnn = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_model_cnn = model_cnn.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4460735e-05, 1.1109689e-07, 1.8177912e-05, 2.0418435e-01,\n",
       "        7.9578292e-01],\n",
       "       [1.9133670e-02, 2.2691553e-02, 9.3409693e-01, 8.6950065e-05,\n",
       "        2.3990940e-02],\n",
       "       [1.4460735e-05, 1.1109689e-07, 1.8177912e-05, 2.0418435e-01,\n",
       "        7.9578292e-01],\n",
       "       ...,\n",
       "       [1.4460763e-05, 1.1109698e-07, 1.8177894e-05, 2.0418443e-01,\n",
       "        7.9578286e-01],\n",
       "       [1.4460763e-05, 1.1109698e-07, 1.8177894e-05, 2.0418443e-01,\n",
       "        7.9578286e-01],\n",
       "       [1.4460763e-05, 1.1109698e-07, 1.8177896e-05, 2.0418443e-01,\n",
       "        7.9578286e-01]], dtype=float32)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using LSTM (Deutsche Lernen accuracy = 97.34%, Weebit Corpus accuracy = 52.61-78.21%,GEO Corpus accuracy=89.47%)\n",
    "# (https://arxiv.org/abs/1607.02501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_lstm  = Sequential()\\nmodel_lstm.add(Embedding(vocabulary_size, 100, input_length=50))\\nmodel_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\\nmodel_lstm.add(Dense(y_train.shape[1], activation=\\'softmax\\'))\\nmodel_lstm.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'rmsprop\\', metrics=[\\'accuracy\\'])\\nprint(\"model fitting - LSTM network\")\\nmodel_lstm.summary()\\n# Training the model\\nmodel_lstm.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\\n# Saving the model\\nmodel_lstm.save(\"simple_lstm.h5\")'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"model_lstm  = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(\"model fitting - LSTM network\")\n",
    "model_lstm.summary()\n",
    "# Training the model\n",
    "model_lstm.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_lstm.save(\"simple_lstm.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_lstm = load_model(\"simple_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting model weights\n",
    "model_lstm_weights = model_lstm.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.57%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_model_lstm = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_simple_lstm = model_lstm.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2080207e-05, 1.2093676e-04, 6.9205234e-05, 2.2278649e-01,\n",
       "        7.7701133e-01],\n",
       "       [1.2080207e-05, 1.2093676e-04, 6.9205234e-05, 2.2278649e-01,\n",
       "        7.7701133e-01],\n",
       "       [1.2080207e-05, 1.2093676e-04, 6.9205234e-05, 2.2278649e-01,\n",
       "        7.7701133e-01],\n",
       "       ...,\n",
       "       [9.2771306e-04, 5.6008477e-02, 9.3685877e-01, 2.3495192e-03,\n",
       "        3.8555253e-03],\n",
       "       [3.0782206e-03, 8.6286885e-01, 1.3114475e-01, 1.3641972e-03,\n",
       "        1.5440936e-03],\n",
       "       [1.1046651e-03, 1.2696024e-02, 9.8238772e-01, 1.4486011e-03,\n",
       "        2.3629030e-03]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_simple_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using convolutional layer on top of the LSTM layer(Deutsch lernen accuracy = 97.90%, Weebit Corpus accuracy =53.15-75.99%,GEO corpus accuracy=85.71%)\n",
    "# (https://arxiv.org/pdf/1511.08630.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdding an one-dimensional CNN and max pooling layers after the Embedding layer which is then feed the \\nconsolidated features to the LSTM unit(to speed up the training proccess)\\n'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adding an one-dimensional CNN and max pooling layers after the Embedding layer which is then feed the \n",
    "consolidated features to the LSTM unit(to speed up the training proccess)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_conv = Sequential()\\nmodel_conv.add(Embedding(vocabulary_size, 100, input_length=50))\\nmodel_conv.add(Dropout(0.2))\\nmodel_conv.add(Conv1D(64, 5, activation=\\'relu\\'))\\nmodel_conv.add(MaxPooling1D(pool_size=4))\\nmodel_conv.add(LSTM(100))\\nmodel_conv.add(Dense(y_train.shape[1], activation=\\'softmax\\'))\\nmodel_conv.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\nprint(\"model fitting - CNN-LSTM convolutional neural network\")\\nmodel_lstm.summary()\\n# Training the model\\nmodel_conv.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)\\n# Saving the model\\nmodel_conv.save(\"lstm_cnn.h5\")'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=4))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"model fitting - CNN-LSTM convolutional neural network\")\n",
    "model_lstm.summary()\n",
    "# Training the model\n",
    "model_conv.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_conv.save(\"lstm_cnn.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_conv = load_model(\"lstm_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.99%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_model_conv = model_conv.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_conv[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_model_conv = model_conv.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2412836e-04, 1.2691332e-04, 1.0445202e-04, 1.3600774e-01,\n",
       "        8.6363673e-01],\n",
       "       [6.6256282e-05, 5.8721198e-05, 1.4250492e-03, 1.4416560e-03,\n",
       "        9.9700838e-01],\n",
       "       [1.2412836e-04, 1.2691332e-04, 1.0445202e-04, 1.3600774e-01,\n",
       "        8.6363673e-01],\n",
       "       ...,\n",
       "       [1.2412813e-04, 1.2691320e-04, 1.0445192e-04, 1.3600767e-01,\n",
       "        8.6363679e-01],\n",
       "       [1.2412813e-04, 1.2691320e-04, 1.0445192e-04, 1.3600767e-01,\n",
       "        8.6363679e-01],\n",
       "       [1.2412813e-04, 1.2691320e-04, 1.0445192e-04, 1.3600767e-01,\n",
       "        8.6363679e-01]], dtype=float32)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Glove word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions. I will be choosing the 100-dimensional version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings from Glove\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using architecture having convolutional layer on top of the LSTM layer along with the globe word embeddings(Accuracy = 75.12%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_glove = Sequential()\\nmodel_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\\nmodel_glove.add(Dropout(0.2))\\nmodel_glove.add(Conv1D(64, 5, activation=\\'relu\\'))\\nmodel_glove.add(MaxPooling1D(pool_size=4))\\nmodel_glove.add(LSTM(100))\\nmodel_glove.add(Dense(y_train.shape[1], activation=\\'softmax\\'))\\nmodel_glove.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\nprint(\"model fitting - CNN-LSTM convolutional neural network with globe word embeddings\")\\nmodel_glove.summary()\\n# Training the model\\nmodel_glove.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)\\n# Saving the model\\nmodel_glove.save(\"lstm_cnn_globe.h5\")'"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"model fitting - CNN-LSTM convolutional neural network with globe word embeddings\")\n",
    "model_glove.summary()\n",
    "# Training the model\n",
    "model_glove.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_glove.save(\"lstm_cnn_globe.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_glove_lstm_cnn = load_model(\"lstm_cnn_globe.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.12%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_model_glove_lstm_cnn = model_glove_lstm_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_glove_lstm_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_model_glove_lstm_cnn = model_glove_lstm_cnn.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4526103e-05, 5.5318833e-05, 1.1670993e-04, 1.0535414e-01,\n",
       "        8.9440936e-01],\n",
       "       [4.2728370e-04, 4.6679538e-04, 2.5911184e-03, 3.9599203e-03,\n",
       "        9.9255484e-01],\n",
       "       [6.4526103e-05, 5.5318833e-05, 1.1670993e-04, 1.0535414e-01,\n",
       "        8.9440936e-01],\n",
       "       ...,\n",
       "       [6.4526044e-05, 5.5318833e-05, 1.1670993e-04, 1.0535414e-01,\n",
       "        8.9440936e-01],\n",
       "       [6.4526044e-05, 5.5318833e-05, 1.1670993e-04, 1.0535414e-01,\n",
       "        8.9440936e-01],\n",
       "       [6.4526044e-05, 5.5318833e-05, 1.1670993e-04, 1.0535414e-01,\n",
       "        8.9440936e-01]], dtype=float32)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model_glove_lstm_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding visialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding weights\n",
    "cnn_embds = model_cnn.layers[0].get_weights()[0]\n",
    "lstm_embds = model_lstm.layers[0].get_weights()[0]\n",
    "conv_embds = model_conv.layers[0].get_weights()[0]\n",
    "glove_emds = model_glove.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word list\n",
    "word_list = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of first two components of TSNE(t-distributed stochastic neighbor embedding)\n",
    "def plot_words(data, start, stop, step):\n",
    "    trace = go.Scatter(\n",
    "        x = data[start:stop:step,0], \n",
    "        y = data[start:stop:step, 1],\n",
    "        mode = 'markers',\n",
    "        text= word_list[start:stop:step]\n",
    "    )\n",
    "    layout = dict(title= 't-SNE 1 vs t-SNE 2',\n",
    "                  yaxis = dict(title='t-SNE 2'),\n",
    "                  xaxis = dict(title='t-SNE 1'),\n",
    "                  hovermode= 'closest')\n",
    "    fig = dict(data = [trace], layout= layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "cnn_tsne_embds = TSNE(n_components=2).fit_transform(cnn_embds)\n",
    "plot_words(cnn_tsne_embds, 0, 2000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "lstm_tsne_embds = TSNE(n_components=2).fit_transform(lstm_embds)\n",
    "plot_words(lstm_tsne_embds, 0, 2000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + LSTM\n",
    "conv_tsne_embds = TSNE(n_components=2).fit_transform(conv_embds)\n",
    "plot_words(conv_tsne_embds, 0, 2000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + LSTM + GLOVE\n",
    "glove_tsne_embds = TSNE(n_components=2).fit_transform(glove_emds)\n",
    "plot_words(glove_tsne_embds, 0, 2000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on geo data and predicting dw data using LSTM network(Accuracy = 99.97%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text data\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Takes an input as a text in string format along and clean it.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"german\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)  \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('german')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus):\n",
    "    \"\"\"\n",
    "    Takes an input as corpus name in string format along with its relative directory path.\n",
    "    \"\"\"\n",
    "    # Loading dataset\n",
    "    df = pd.read_hdf(corpus,\"text_df\")[['text','y']]\n",
    "    # Dropping null values\n",
    "    df.dropna(inplace=True)\n",
    "    # Converting class labels to int dtype\n",
    "    df['y'] = df['y'].astype(int)\n",
    "    print(\"Shape of the dataset:\",df.shape)\n",
    "    df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "    print(\"Dataset:\",df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sequence(df,vocabulary_size = 20000):\n",
    "    \"\"\"\n",
    "    Takes the dataframe, and returns the tokenized-sequenced features and labels.\n",
    "    \"\"\"\n",
    "    # Because of the computational expenses, I am using the top 20000 unique words. \n",
    "    # At first, the text is tokenized and then convert those into sequences. \n",
    "    # I have kept 50 words to limit the number of words in each comment.\n",
    "\n",
    "    # Initializing Tokenizer from keras\n",
    "    tokenizer = Tokenizer(nb_words=vocabulary_size)\n",
    "    # Fitting text on the tokenizer\n",
    "    tokenizer.fit_on_texts(df['text'])\n",
    "    # Converting text to sequence\n",
    "    sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "    # Finding unique tokenizer\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # Padding sequences to the length of MAX_SEQUENCE_LENGTH\n",
    "    data = pad_sequences(sequences, maxlen=50)\n",
    "    labels = df['y']\n",
    "    \n",
    "    #performing one hot encoding on the labels\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(corpus1_data,corpus1_labels,corpus2_data,corpus2_labels):\n",
    "    \"\"\"\n",
    "    Takes the labels and data from both the corpuses and considers the first corpus as the training corpus and \n",
    "    the second one as the testing corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the labels and features data\n",
    "    print('Shape of Corpus1 data tensor:', corpus1_data.shape)\n",
    "    print('Shape of Corpus1 label tensor:', corpus1_labels.shape)\n",
    "    \n",
    "    print('Shape of Corpus2 data tensor:', corpus2_data.shape)\n",
    "    print('Shape of Corpus2 label tensor:', corpus2_labels.shape)\n",
    "\n",
    "\n",
    "    X_train = corpus1_data\n",
    "    y_train = corpus1_labels\n",
    "    X_test = corpus2_data\n",
    "    y_test = corpus2_labels\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (2130, 2)\n",
      "Dataset:                                                 text  y\n",
      "0  kologie: dauerleck nordsee. seit jahr sprudelt...  1\n",
      "1  fischzug tiefe. herb vergang jahr brach polars...  1\n",
      "2  artikel 14.2.2004. landwirtschaft: geplatzt ge...  1\n",
      "3  artikel 28.8.2006. planet verschwind l sst ast...  1\n",
      "4  artikel 20.12.2010. verspielt seel wen igelf s...  1\n",
      "Found 77216 unique tokens.\n",
      "Shape of the dataset: (7814, 2)\n",
      "Dataset:                                                 text  y\n",
      "0  us-pr sident barack obama 250 zus tzlich solda...  1\n",
      "1  us-pr sident barack obama montag seit kanzleri...  1\n",
      "2  rechtspopulist fp bundespr sidentenwahl sterre...  1\n",
      "3  deutschland zahl fl chtling maghreb-staat alge...  1\n",
      "4  beid abgeschlag pr sidentschaftskandidat us-re...  1\n",
      "Found 49565 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Loading and preprocessing data\n",
    "geo_df = preprocess_data(\"geo.h5\")\n",
    "geo_data,geo_labels = tokenize_sequence(geo_df)\n",
    "\n",
    "dw_df = preprocess_data(\"dw.hdf5\")\n",
    "dw_data,dw_labels = tokenize_sequence(dw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Corpus1 data tensor: (2130, 50)\n",
      "Shape of Corpus1 label tensor: (2130, 2)\n",
      "Shape of Corpus2 data tensor: (7814, 50)\n",
      "Shape of Corpus2 label tensor: (7814, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train and test corpus\n",
    "X_train,y_train,X_test,y_test = split_data(geo_data,geo_labels,dw_data,dw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_lstm  = Sequential()\\nmodel_lstm.add(Embedding(vocabulary_size, 100, input_length=50))\\nmodel_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\\nmodel_lstm.add(Dense(y_train.shape[1], activation=\\'softmax\\'))\\nmodel_lstm.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'rmsprop\\', metrics=[\\'accuracy\\'])\\nprint(\"model fitting - LSTM network\")\\nmodel_lstm.summary()\\n# Training the model\\nmodel_lstm.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\\n# Saving the model\\nmodel_lstm.save(\"split_simple_geo_dw_lstm.h5\")'"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"model_lstm  = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(\"model fitting - LSTM network\")\n",
    "model_lstm.summary()\n",
    "# Training the model\n",
    "model_lstm.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_lstm.save(\"split_simple_geo_dw_lstm.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_lstm = load_model(\"split_simple_geo_dw_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.22%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_model_lstm = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_simple_lstm = model_lstm.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the data in form a histogram\n",
    "def plot_histogram(data,labels):\n",
    "    \"\"\"\n",
    "    Takes the probabilities in  the form a list and class labels as unique labels with values belonging to each of the \n",
    "    classes and plots a histogram over it.\n",
    "    \"\"\"\n",
    "    # Getting class prbabilities for every text in the corpus\n",
    "    class_labels = []\n",
    "    colors = [\"blue\",\"yellow\",\"red\",\"green\",\"black\"]\n",
    "    color_labels = [\"Class label 0\",\"Class label 1\",\"Class label 2\",\"Class label 3\",\"Class label 4\"]\n",
    "\n",
    "    for arrays in data:\n",
    "        for index,classes in enumerate(labels):\n",
    "            class_labels.append(arrays[index])\n",
    "        \n",
    "    # on the same Axis\n",
    "    colors = colors[:len(labels)]\n",
    "    color_labels = color_labels[:len(labels)]\n",
    "    for index,classes in enumerate(class_labels):\n",
    "        sns.distplot(classes , color=colors[index], label=color_labels[index],kde=False)\n",
    "        plt.savefig(\"pred_prob_curve.jpg\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution LSTM predictions:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d75615160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Probability distribution LSTM predictions:\")\n",
    "plot_histogram(prediction_simple_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on dw data and predicting geo data using LSTM network(Accuracy = 50.47%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Corpus1 data tensor: (7814, 50)\n",
      "Shape of Corpus1 label tensor: (7814, 2)\n",
      "Shape of Corpus2 data tensor: (2130, 50)\n",
      "Shape of Corpus2 label tensor: (2130, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train and test corpus\n",
    "X_train,y_train,X_test,y_test = split_data(dw_data,dw_labels,geo_data,geo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "\"\"\"# Configuring the neural network\n",
    "model_lstm  = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(\"model fitting - LSTM network\")\n",
    "model_lstm.summary()\n",
    "# Training the model\n",
    "model_lstm.fit(X_train,y_train,validation_data=(X_test, y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_lstm.save(\"split_simple_dw_geo_lstm.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.47%\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained model\n",
    "model_lstm = load_model(\"split_simple_dw_geo_lstm.h5\")\n",
    "\n",
    "# Checking the accuracy\n",
    "accuracy_model_lstm = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_model_lstm[1]*100))\n",
    "\n",
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_simple_lstm = model_lstm.predict(X_test,batch_size=10,verbose=0)# Getting class prbabilities for every text in the corpus\n",
    "\n",
    "class_0,class_1 = ([] for i in range(2))\n",
    "for index,arrays in enumerate(prediction_simple_lstm):\n",
    "    class_0.append(arrays[0])\n",
    "    class_1.append(arrays[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability distribution LSTM predictions:\")\n",
    "plot_histogram(prediction_simple_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Transfer learning on Weebit pre-trained Deutsche learnen Corpus(Using LSTM network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (7814, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-Prsident Barack Obama will bis zu 250 zus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-Prsident Barack Obama wird an diesem Monta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die rechtspopulistische FP hat bei der Bundes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Deutschland ist die Zahl der Flchtlinge au...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die beiden abgeschlagenen Prsidentschaftskand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y\n",
       "0  US-Prsident Barack Obama will bis zu 250 zus...  1\n",
       "1  US-Prsident Barack Obama wird an diesem Monta...  1\n",
       "2  Die rechtspopulistische FP hat bei der Bundes...  1\n",
       "3  In Deutschland ist die Zahl der Flchtlinge au...  1\n",
       "4  Die beiden abgeschlagenen Prsidentschaftskand...  1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading necessary libraries\n",
    "dw_df = pd.read_hdf(\"dw.hdf5\",\"text_df\")[['text','y']]\n",
    "# Dropping null values\n",
    "dw_df.dropna(inplace=True)\n",
    "# Converting class labels to int dtype\n",
    "dw_df['y'] = dw_df['y'].astype(int)\n",
    "print(\"Shape of the dataset:\",dw_df.shape)\n",
    "dw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60833 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Because of the computational expenses, I am using the top 20000 unique words. \n",
    "# At first, the text is tokenized and then convert those into sequences. \n",
    "# I have kept 50 words to limit the number of words in each comment.\n",
    "vocabulary_size = 20000\n",
    "\n",
    "# Initializing Tokenizer from keras\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "# Fitting text on the tokenizer\n",
    "tokenizer.fit_on_texts(dw_df['text'])\n",
    "# Converting text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(dw_df['text'])\n",
    "\n",
    "# Finding unique tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Padding sequences to the length of MAX_SEQUENCE_LENGTH\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "labels = dw_df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels from the corpora: [1 0]\n",
      "Total no of sentences in the Corpora is 7349\n",
      "No of sentences in class label 1 is 6753\n",
      "No of sentences in class label 0 is 1061\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e2d1be1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_class_labels = dw_df['y'].unique()\n",
    "print(\"class labels from the corpora:\",unique_class_labels)\n",
    "\n",
    "# Used to modify elements of a dictionary\n",
    "class make_dict(dict):\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "\n",
    "# Printing the total number of sentences in the corpora\n",
    "print(\"Total no of sentences in the Corpora is\",df.shape[0])\n",
    "# Getting count of sentences for every class\n",
    "count_list = []\n",
    "for class_label in dw_df['y'].unique():\n",
    "    print(\"No of sentences in class label\",str(class_label) + \" is \" + str(dw_df[dw_df['y']==class_label].shape[0]))\n",
    "    count_list.append((str(class_label),int(dw_df[dw_df['y']==class_label].shape[0])))\n",
    "\n",
    "# Getting data into dictionary for plotting\n",
    "no_of_sentence = make_dict()\n",
    "for values in count_list:\n",
    "    no_of_sentence[values[0]] = values[1]\n",
    "\n",
    "plt.bar(list(no_of_sentence.keys()), no_of_sentence.values(), color='g')\n",
    "plt.suptitle(no_of_sentence.keys(), fontsize=10)\n",
    "plt.xlabel('Class labels', fontsize=10)\n",
    "plt.ylabel('No of Sentences', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (7814, 50)\n",
      "Shape of label tensor: (7814, 2)\n"
     ]
    }
   ],
   "source": [
    "# Getting the labels and features data\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Getting the splitting index for training and testing data\n",
    "SPLIT_RATIO = 0.25\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "validation_samples = int(SPLIT_RATIO * data.shape[0])\n",
    "\n",
    "# Getting the testing and training dataset\n",
    "X_train = data[:-validation_samples]\n",
    "y_train = labels[:-validation_samples]\n",
    "X_test = data[-validation_samples:]\n",
    "y_test = labels[-validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LSTM model on Deutche lernen corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - CNN-LSTM convolutional neural network with globe word embeddings\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 50, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 46, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,098,266\n",
      "Trainable params: 2,098,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5861 samples, validate on 1953 samples\n",
      "Epoch 1/10\n",
      "5861/5861 [==============================] - 15s 3ms/step - loss: 0.2275 - acc: 0.9133 - val_loss: 0.0780 - val_acc: 0.9734\n",
      "Epoch 2/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 0.0270 - acc: 0.9915 - val_loss: 0.1019 - val_acc: 0.9703\n",
      "Epoch 3/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1117 - val_acc: 0.9698\n",
      "Epoch 4/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 8.8293e-05 - acc: 1.0000 - val_loss: 0.1261 - val_acc: 0.9683\n",
      "Epoch 5/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 4.0265e-05 - acc: 1.0000 - val_loss: 0.1394 - val_acc: 0.9683\n",
      "Epoch 6/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 2.1478e-05 - acc: 1.0000 - val_loss: 0.1489 - val_acc: 0.9677\n",
      "Epoch 7/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 1.4337e-05 - acc: 1.0000 - val_loss: 0.1561 - val_acc: 0.9667\n",
      "Epoch 8/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 9.9806e-06 - acc: 1.0000 - val_loss: 0.1631 - val_acc: 0.9667\n",
      "Epoch 9/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 8.8224e-06 - acc: 1.0000 - val_loss: 0.1687 - val_acc: 0.9667\n",
      "Epoch 10/10\n",
      "5861/5861 [==============================] - 13s 2ms/step - loss: 7.0232e-06 - acc: 1.0000 - val_loss: 0.1745 - val_acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "# Configuring the neural network\n",
    "model_transfer = Sequential()\n",
    "model_transfer.add(Embedding(vocabulary_size, 100, input_length=50, weights=[np.array(model_lstm_weights[0])], trainable=True))\n",
    "model_transfer.add(Dropout(0.2))\n",
    "model_transfer.add(Conv1D(64, 5, activation='relu'))\n",
    "model_transfer.add(MaxPooling1D(pool_size=4))\n",
    "model_transfer.add(LSTM(100))\n",
    "model_transfer.add(Dense(2, activation='softmax'))\n",
    "model_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"model fitting - CNN-LSTM convolutional neural network with globe word embeddings\")\n",
    "model_transfer.summary()\n",
    "# Training the model\n",
    "model_transfer.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)\n",
    "# Saving the model\n",
    "model_transfer.save(\"lstm_cnn_transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_transfer = load_model(\"lstm_cnn_transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_transfer_lstm_cnn = model_transfer.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_transfer_lstm_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_model_transfer = model_transfer.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0169004e-04, 9.9989831e-01],\n",
       "       [2.8252684e-02, 9.7174734e-01],\n",
       "       [8.2518528e-07, 9.9999917e-01],\n",
       "       ...,\n",
       "       [1.1015429e-06, 9.9999893e-01],\n",
       "       [4.7569301e-06, 9.9999523e-01],\n",
       "       [3.0246863e-06, 9.9999702e-01]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
